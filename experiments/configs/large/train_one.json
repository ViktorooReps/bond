{
    "output": "logs/based.log",
    "config_name": "based",
    "memory_needed": 20.0,
    "command": "python train.py",
    "args": [
        "--k_folds=4",
        "--base_model=bond",
        "--seed=42",
        "--add_gold_labels=0.50",
        "--use_adaptive_scheduler",
        "--logging=100",
        "--adaptive_scheduler_patience=5",
        "--framework=bond",
        "--resfile=res.final.txt",
        "--dataset=conll03",
        "--dataset_type=gold/train",
        "--model_type=roberta",
        "--model_name=roberta-large",
        "--gradient_accumulation_steps=1",
        "--max_seq_len=512",
        "--batch_size=4",
        "--head_dropout=0.3",
        "--bert_dropout=0.1",
        "--learning_rate=5e-05",
        "--weight_decay=1e-04",
        "--adam_epsilon=1e-06",
        "--adam_beta1=0.9",
        "--adam_beta2=0.999",
        "--no_entity_weight=0.7",
        "--warmup_proportion=0.2",
        "--ner_fit_epochs=10",
        "--self_training_lr_proportion=0.2",
        "--correct_frequency",
        "--use_kldiv_loss",
        "--label_keep_threshold=0.9",
        "--self_training_epochs=15",
        "--start_updates=2.0",
        "--end_updates=1.0"
    ]
}

python train.py --seed=42 --use_adaptive_scheduler --logging=100 --adaptive_scheduler_patience=5 --framework=supervised --resfile=res.final.txt --dataset=conll03 --dataset_type=gold/train --model_type=roberta --model_name=roberta-large --gradient_accumulation_steps=1 --max_seq_len=512 --batch_size=4 --head_dropout=0.3 --bert_dropout=0.1 --learning_rate=5e-05 --weight_decay=1e-04 --adam_epsilon=1e-06 --adam_beta1=0.9 --adam_beta2=0.999 --no_entity_weight=0.7 --warmup_proportion=0.2 --ner_fit_epochs=10
