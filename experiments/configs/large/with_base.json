{
    "output": "logs/based.log",
    "config_name": "based",
    "memory_needed": 7.0,
    "command": "python train.py",
    "args": [
        "--k_folds=4",
        "--base_model=bond",
        "--use_kldiv_loss_ner",
        ["--seed=1", "--seed=2", "--seed=3", "--seed=4", "--seed=5"],
        ["--add_gold_labels=0.05", "--add_gold_labels=0.10", "--add_gold_labels=0.15", "--add_gold_labels=0.20",
          "--add_gold_labels=0.30", "--add_gold_labels=0.40", "--add_gold_labels=0.50"],
        "--use_adaptive_scheduler",
        "--logging=100",
        "--adaptive_scheduler_patience=5",
        "--framework=bond",
        "--resfile=res.final.txt",
        "--dataset=conll03",
        "--dataset_type=distant/train",
        "--model_type=roberta",
        "--model_name=roberta-large",
        "--gradient_accumulation_steps=1",
        "--max_seq_len=512",
        "--batch_size=4",
        "--head_dropout=0.3",
        "--bert_dropout=0.1",
        "--learning_rate=5e-05",
        "--weight_decay=1e-04",
        "--adam_epsilon=1e-06",
        "--adam_beta1=0.9",
        "--adam_beta2=0.999",
        "--no_entity_weight=0.7",
        "--warmup_proportion=0.2",
        "--ner_fit_epochs=10",
        "--self_training_lr_proportion=0.2",
        "--correct_frequency",
        "--use_kldiv_loss",
        "--label_keep_threshold=0.9",
        "--self_training_epochs=15",
        "--start_updates=2.0",
        "--end_updates=1.0"
    ]
}