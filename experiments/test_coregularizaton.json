[
  {
    "script": "train.py",
    "save_output": "logs/test-coregularization-100/script_output.log",
    "args": [
      "--framework=supervised",
      "--experiment_name=test-coregularization-100",
      "--resfile=res.txt",
      "--dataset=conll03",
      "--dataset_type=distant/train",
      "--model_type=roberta",
      "--model_name=roberta-base",
      "--gradient_accumulation_steps=16",
      "--max_seq_len=196",
      "--batch_size=1",
      "--head_dropout=0.7",
      "--bert_dropout=0.1",
      "--head_learning_rate=5e-05",
      "--bert_learning_rate=5e-05",
      "--weight_decay=1e-04",
      "--adam_epsilon=1e-06",
      "--adam_beta1=0.9",
      "--adam_beta2=0.999",
      "--use_nll",
      "--agreement_strength=100.0",
      "--warmup_proportion=0.1",
      "--ner_fit_epochs=6",
      "--use_linear_scheduler",
      "--seed=42"
    ]
  },
  {
    "script": "train.py",
    "save_output": "logs/test-coregularization-1000/script_output.log",
    "args": [
      "--framework=supervised",
      "--experiment_name=test-coregularization-1000",
      "--resfile=res.txt",
      "--dataset=conll03",
      "--dataset_type=distant/train",
      "--model_type=roberta",
      "--model_name=roberta-base",
      "--gradient_accumulation_steps=16",
      "--max_seq_len=196",
      "--batch_size=1",
      "--head_dropout=0.7",
      "--bert_dropout=0.1",
      "--head_learning_rate=5e-05",
      "--bert_learning_rate=5e-05",
      "--weight_decay=1e-04",
      "--adam_epsilon=1e-06",
      "--adam_beta1=0.9",
      "--adam_beta2=0.999",
      "--use_nll",
      "--agreement_strength=1000.0",
      "--warmup_proportion=0.1",
      "--ner_fit_epochs=6",
      "--use_linear_scheduler",
      "--seed=42"
    ]
  },
  {
    "script": "train.py",
    "save_output": "logs/test-coregularization-500/script_output.log",
    "args": [
      "--framework=supervised",
      "--experiment_name=test-coregularization-500",
      "--resfile=res.txt",
      "--dataset=conll03",
      "--dataset_type=distant/train",
      "--model_type=roberta",
      "--model_name=roberta-base",
      "--gradient_accumulation_steps=16",
      "--max_seq_len=196",
      "--batch_size=1",
      "--head_dropout=0.7",
      "--bert_dropout=0.1",
      "--head_learning_rate=5e-05",
      "--bert_learning_rate=5e-05",
      "--weight_decay=1e-04",
      "--adam_epsilon=1e-06",
      "--adam_beta1=0.9",
      "--adam_beta2=0.999",
      "--use_nll",
      "--agreement_strength=500.0",
      "--warmup_proportion=0.1",
      "--ner_fit_epochs=6",
      "--use_linear_scheduler",
      "--seed=42"
    ]
  }
]